@book{CameronTrivedi2013,
  author    = {A. Colin Cameron and Pravin K. Trivedi},
  title     = {Regression Analysis of Count Data},
  year      = {2013},
  edition   = {2nd},
  publisher = {Cambridge University Press},
  address   = {Cambridge}
}

@book{ChambersHastie1992,
  editor    = {John M. Chambers and Trevor J. Hastie},
  title     = {Statistical Models in S},
  publisher = {Chapman \& Hall},
  year      = {1992},
  address   = {London}
}

@manual{Jackman2015,
  title  = {pscl: Classes and Methods for R Developed in the Political Science Computational Laboratory, Stanford University},
  author = {Simon Jackman},
  year   = {2015},
  note   = {R package version 1.4.9},
  url    = {https://CRAN.R-project.org/package=pscl}
}

@article{Mullahy1986,
  author  = {John Mullahy},
  title   = {Specification and Testing of Some Modified Count Data Models},
  year    = {1986},
  journal = {Journal of Econometrics},
  volume  = {33},
  number  = {3},
  pages   = {341--365},
  doi     = {10.1016/0304-4076(86)90002-3}
}

@book{McCullaghNelder1989,
  author    = {Peter McCullagh and John A. Nelder},
  title     = {Generalized Linear Models},
  edition   = {2nd},
  year      = {1989},
  publisher = {Chapman \& Hall},
  address   = {London},
  doi       = {10.1007/978-1-4899-3242-6}
}

@Article{Nakagawa2017,
author = {Nakagawa, S et al.},
title = {Meta-evaluation of meta-analysis: ten appraisal questions for biologists.},
journal = {BMC Biol},
volume = {15},
number = {1},
pages = {18},
year = {2017},
abstract = {Meta-analysis is a statistical procedure for analyzing the combined data from different studies, and can be a major source of concise up-to-date information. The overall conclusions of a meta-analysis, however, depend heavily on the quality of the meta-analytic process, and an appropriate evaluation of the quality of meta-analysis (meta-evaluation) can be challenging. We outline ten questions biologists can ask to critically appraise a meta-analysis. These questions could also act as simple and accessible guidelines for the authors of meta-analyses. We focus on meta-analyses using non-human species, which we term `biological' meta-analysis. Our ten questions are aimed at enabling a biologist to evaluate whether a biological meta-analysis embodies `mega-enlightenment', a `mega-mistake', or something in between.},
location = {Evolution \& Ecology Research Centre and School of Biological, Earth and Environmental Sciences, University of New South Wales, Sydney, NSW, 2052, Australia. s.nakagawa@unsw.edu.au. Diabetes and Metabolism Division, Garvan Institute of Medical Research, 384 Victoria Street, Darlinghurst, Sydney, NSW, 2010, Australia. s.nakagawa@unsw.edu.au. Evolution \& Ecology Research Centre and School of Biological, Earth and Environmental Sciences, University of New South Wales, Sydney, NSW, 2052, Australia. Charles Perkins Centre, University of Sydney, Sydney, NSW, 2006, Australia. School of Mathematics and Statistics, University of Sydney, Sydney, NSW, 2006, Australia. Evolution \& Ecology Research Centre and School of Biological, Earth and Environmental Sciences, University of New South Wales, Sydney, NSW, 2052, Australia.},
keywords = {}}



@manual{R,
  title        = {R: {A} Language and Environment for Statistical Computing},
  author       = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address      = {Vienna, Austria},
  year         = {2017},
  url          = {https://www.R-project.org/}
}

@article{StasinopoulosRigby2007,
  author  = {D. Mikis Stasinopoulos and Robert A. Rigby},
  title   = {Generalized Additive Models for Location Scale and Shape ({GAMLSS}) in R},
  journal = {Journal of Statistical Software},
  year    = {2007},
  volume  = {23},
  number  = {7},
  pages   = {1--46},
  doi     = {10.18637/jss.v023.i07}
}

@book{VenablesRipley2002,
  author    = {William N. Venables and Brian D. Ripley},
  title     = {Modern Applied Statistics with S},
  edition   = {4th},
  year      = {2002},
  pages     = {495},
  publisher = {Springer-Verlag},
  address   = {New York},
  doi       = {10.1007/978-0-387-21706-2}
}

@book{Wood2006,
  author    = {Simon N. Wood},
  title     = {Generalized Additive Models: An Introduction with R},
  year      = {2006},
  publisher = {Chapman \& Hall/CRC},
  address   = {Boca Raton}
}

@article{Yee2009,
  author  = {Thomas W. Yee},
  title   = {The VGAM Package for Categorical Data Analysis},
  journal = {Journal of Statistical Software},
  year    = {2010},
  volume  = {32},
  number  = {10},
  pages   = {1--34},
  doi     = {10.18637/jss.v032.i10}
}

@article{ZeileisKleiberJackman2008,
  author  = {Achim Zeileis and Christian Kleiber and Simon Jackman},
  title   = {Regression Models for Count Data in R},
  journal = {Journal of Statistical Software},
  year    = {2008},
  volume  = {27},
  number  = {8},
  pages   = {1--25},
  doi     = {10.18637/jss.v027.i08}
}


@article{hemming2017,
	title = {A practical guide to structured expert elicitation using the IDEA protocol},
	author = {{Hemming}, {Victoria} and {Burgman}, {Mark A.} and {Hanea}, {Anca M.} and {McBride}, {Marissa F.} and {Wintle}, {Bonnie C.}},
	editor = {{Anderson}, {Barbara}},
	year = {2017},
	month = {09},
	date = {2017-09-05},
	journal = {Methods in Ecology and Evolution},
	pages = {169--180},
	volume = {9},
	number = {1},
	doi = {10.1111/2041-210x.12857},
	url = {http://dx.doi.org/10.1111/2041-210x.12857},
	langid = {en}
}

@Article{Gordon2020,
author = {Gordon, Michael  and Viganola, Domenico  and Bishop, Michael  and Chen, Yiling  and Dreber, Anna  and Goldfedder, Brandon  and Holzmeister, Felix  and Johannesson, Magnus  and Liu, Yang  and Twardy, Charles  and Wang, Juntao  and Pfeiffer, Thomas },
title = {Are replication rates the same across academic fields? Community forecasts from the DARPA SCORE programme},
journal = {Royal Society Open Science},
volume = {7},
number = {7},
pages = {200566},
year = {2020},
doi = {10.1098/rsos.200566},
    abstract = { The Defense Advanced Research Projects Agency (DARPA) programme ‘Systematizing Confidence in Open Research and Evidence' (SCORE) aims to generate confidence scores for a large number of research claims from empirical studies in the social and behavioural sciences. The confidence scores will provide a quantitative assessment of how likely a claim will hold up in an independent replication. To create the scores, we follow earlier approaches and use prediction markets and surveys to forecast replication outcomes. Based on an initial set of forecasts for the overall replication rate in SCORE and its dependence on the academic discipline and the time of publication, we show that participants expect replication rates to increase over time. Moreover, they expect replication rates to differ between fields, with the highest replication rate in economics (average survey response 58\%), and the lowest in psychology and in education (average survey response of 42\% for both fields). These results reveal insights into the academic community's views of the replication crisis, including for research fields for which no large-scale replication studies have been undertaken yet. }
}

@Article{Hanea2021,
 title={Mathematically aggregating experts' predictions of possible futures},
 DOI={https://doi.org/10.1371/journal.pone.0256919},
 journal={PLoS ONE},
  author = {Hanea, Anca and Wilkinson, David P and McBride, Marissa and Lyon, Aidan and van Ravenzwaaij, Don and Singleton Thorn, Felix and Gray, Charles T and Mandel, David R and Willcox, Aaron and Gould, Elliot and et al.},
 year={2021},
 month={September},
 volume = {16},
 number = {9}
}

@misc{Gould2022,
 title={aggreCAT: An R Package for Mathematically Aggregating Expert Judgments},
 DOI={10.31222/osf.io/74tfv},
 publisher={MetaArXiv},
 author={Gould, Elliot and Gray, Charles T and Willcox, Aaron and O'Dea, Rose E and Groenewegen, Rebecca and Wilkinson, David P},
 year={in prep.},
 month={Apr}
}

@Manual{R2JAGS,
title = {R2jags: Using R to Run 'JAGS'},
author = {Yu-Sung Su and Masanao Yajima},
year = {2020},
note = {R package version 0.6-1},
url = {https://CRAN.R-project.org/package=R2jags}
}

@Article{Pearson2021,
author = {Pearson, Ross and Fraser, Hannah and Bush, Martin and Mody, Fallon and Widjaja, Ivo and Head, Andy and Wilkinson, David Peter and Sinnott, Richard and Wintle, Bonnie and Burgman, Mark and Fidler, Fiona and Vesk, Peter},
editor = {},
title = {Eliciting group judgements about replicability: a technical implementation of the IDEA Protocol},
booktitle = {Eliciting group judgements about replicability: a technical implementation of the IDEA Protocol},
volume = {},
publisher = {Hawaii International Conference on System Sciences},
address = {},
pages = {},
year = {2021},
abstract = {},
keywords = {},
url = {http://hdl.handle.net/10125/70666}}

@Article{Wickham:2014vp,
author = {Wickham, H},
title = {Tidy data},
journal = {Journal of Statistical Software},
volume = {59},
number = {10},
pages = {},
year = {2014},
abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
location = {},
keywords = {R; software}}


@Article{Wintle:2021,
 title={Predicting and reasoning about replicability using structured groups},
 DOI={10.31222/osf.io/vtpmb},
 publisher={MetaArXiv},
 author={Wintle, Bonnie and Mody, Fallon and Smith, Eden T and Hanea, Anca and Wilkinson, David P. and Hemming, Victoria and Bush, Martin and Fraser, Hannah and Singleton Thorn, Felix and McBride, Marissa and Gould, Elliot and Head, Andrew and Hamilton, Dan and Rumpff, Libby and Hoekstra, Rink and Fidler, Fiona},
 year={2021},
 month={May}
}


@article{Sutherland2018,
author = {Sutherland, William J. and Dicks, Lynn V. and Everard, Mark and Geneletti, Davide},
title = {Qualitative methods for ecologists and conservation scientists},
journal = {Methods in Ecology and Evolution},
volume = {9},
number = {1},
pages = {7-9},
keywords = {expert elicitation, focus groups, multi-criteria decision analysis, nominal group technique, policy making, qualitative interviews, qualitative research},
doi = {https://doi.org/10.1111/2041-210X.12956},
abstract = {Abstract Conservation of biodiversity involves dealing with problems caused by humans, by applying solutions that comprise actions by humans. Understanding human attitudes, knowledge and behaviour are thus central to conservation research and practice. The special feature brings together authors from a range of disciplines (ecology, human geography, political science, land economy, management) to examine a set of qualitative techniques used in conservation research: Interviews, Focus group discussion, The Nominal Group Technique and multi-criteria decision analysis. These techniques can be used for a range of purposes—most notably to understand people's perspectives, values and attitudes and to gather information about approaches to management of species, ecosystems or natural resources. Incorporating human values, perceptions, judgements and knowledge into conservation decision making is an important role for qualitative techniques; they provide robust means for submitting this information or knowledge as evidence. The articles in this special feature highlight a worrying extent of poor justification and inadequate reporting of qualitative methods in the conservation literature. To improve and encourage greater use of these techniques in conservation science, we urge improved reporting of rationales and methods, along with innovation, adaptation and further testing of the methods themselves.},
year = {2018}
}

@article{Klein2014,
author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and {Adams Jr.}, Reginald B. and Bahnic, Stepan and Bernstein, Michael J. and Bocian, Konrad and Brandt, {Mark. J.} and Brooks, Beach and Brumbaugh, Claudia Chloe and Cermalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galiani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, {James F.} and Hunt, {S. Jane} and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and Joy-Gaba, Jennifer A. and {Barry Kappes}, Heather and Kreuger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallet, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and {Van Swol}, Lyn M. and Thompson, Donna and {van 't Veer}, A.E. and Vaughn, Leigh Ann and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
title = {Investigating Variation in Replicability},
journal = {Social Psychology},
volume = {45},
number = {3},
pages = {142–152},
year = {2014},
abstract = {},
location = {},
keywords = {}}


@article{Klein2018ManyL2,
author = {Richard A. Klein and Michelangelo Vianello and Fred Hasselman and Byron G. Adams and Reginald B. Adams, Jr. and Sinan Alper and Mark Aveyard and Jordan R. Axt and Mayowa T. Babalola and Štěpán Bahník and Rishtee Batra and Mihály Berkics and Michael J. Bernstein and Daniel R. Berry and Olga Bialobrzeska and Evans Dami Binan and Konrad Bocian and Mark J. Brandt and Robert Busching and Anna Cabak Rédei and Huajian Cai and Fanny Cambier and Katarzyna Cantarero and Cheryl L. Carmichael and Francisco Ceric and Jesse Chandler and Jen-Ho Chang and Armand Chatard and Eva E. Chen and Winnee Cheong and David C. Cicero and Sharon Coen and Jennifer A. Coleman and Brian Collisson and Morgan A. Conway and Katherine S. Corker and Paul G. Curran and Fiery Cushman and Zubairu K. Dagona and Ilker Dalgar and Anna Dalla Rosa and William E. Davis and Maaike de Bruijn and Leander De Schutter and Thierry Devos and Marieke de Vries and Canay Doğulu and Nerisa Dozo and Kristin Nicole Dukes and Yarrow Dunham and Kevin Durrheim and Charles R. Ebersole and John E. Edlund and Anja Eller and Alexander Scott English and Carolyn Finck and Natalia Frankowska and Miguel-Ángel Freyre and Mike Friedman and Elisa Maria Galliani and Joshua C. Gandi and Tanuka Ghoshal and Steffen R. Giessner and Tripat Gill and Timo Gnambs and Ángel Gómez and Roberto González and Jesse Graham and Jon E. Grahe and Ivan Grahek and Eva G. T. Green and Kakul Hai and Matthew Haigh and Elizabeth L. Haines and Michael P. Hall and Marie E. Heffernan and Joshua A. Hicks and Petr Houdek and Jeffrey R. Huntsinger and Ho Phi Huynh and Hans IJzerman and Yoel Inbar and Åse H. Innes-Ker and William Jiménez-Leal and Melissa-Sue John and Jennifer A. Joy-Gaba and Roza G. Kamiloğlu and Heather Barry Kappes and Serdar Karabati and Haruna Karick and Victor N. Keller and Anna Kende and Nicolas Kervyn and Goran Knežević and Carrie Kovacs and Lacy E. Krueger and German Kurapov and Jamie Kurtz and Daniël Lakens and Ljiljana B. Lazarević and Carmel A. Levitan and Neil A. Lewis, Jr. and Samuel Lins and Nikolette P. Lipsey and Joy E. Losee and Esther Maassen and Angela T. Maitner and Winfrida Malingumu and Robyn K. Mallett and Satia A. Marotta and Janko Međedović and Fernando Mena-Pacheco and Taciano L. Milfont and Wendy L. Morris and Sean C. Murphy and Andriy Myachykov and Nick Neave and Koen Neijenhuijs and Anthony J. Nelson and Félix Neto and Austin Lee Nichols and Aaron Ocampo and Susan L. O’Donnell and Haruka Oikawa and Masanori Oikawa and Elsie Ong and Gábor Orosz and Malgorzata Osowiecka and Grant Packard and Rolando Pérez-Sánchez and Boban Petrović and Ronaldo Pilati and Brad Pinter and Lysandra Podesta and Gabrielle Pogge and Monique M. H. Pollmann and Abraham M. Rutchick and Patricio Saavedra and Alexander K. Saeri and Erika Salomon and Kathleen Schmidt and Felix D. Schönbrodt and Maciej B. Sekerdej and David Sirlopú and Jeanine L. M. Skorinko and Michael A. Smith and Vanessa Smith-Castro and Karin C. H. J. Smolders and Agata Sobkow and Walter Sowden and Philipp Spachtholz and Manini Srivastava and Troy G. Steiner and Jeroen Stouten and Chris N. H. Street and Oskar K. Sundfelt and Stephanie Szeto and Ewa Szumowska and Andrew C. W. Tang and Norbert Tanzer and Morgan J. Tear and Jordan Theriault and Manuela Thomae and David Torres and Jakub Traczyk and Joshua M. Tybur and Adrienn Ujhelyi and Robbie C. M. van Aert and Marcel A. L. M. van Assen and Marije van der Hulst and Paul A. M. van Lange and Anna Elisabeth van ’t Veer and Alejandro Vásquez- Echeverría and Leigh Ann Vaughn and Alexandra Vázquez and Luis Diego Vega and Catherine Verniers and Mark Verschoor and Ingrid P. J. Voermans and Marek A. Vranka and Cheryl Welch and Aaron L. Wichman and Lisa A. Williams and Michael Wood and Julie A. Woodzicka and Marta K. Wronska and Liane Young and John M. Zelenski and Zeng Zhijia and Brian A. Nosek},
title ={Many Labs 2: Investigating Variation in Replicability Across Samples and Settings},
journal = {Advances in Methods and Practices in Psychological Science},
volume = {1},
number = {4},
pages = {443-490},
year = {2018},
doi = {10.1177/2515245918810225},
    abstract = { We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p < .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p < .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen’s ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small (< 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied. }
}





@Article{Gould2021a,
 title={Using model-based predictions to inform the mathematical aggregation of human-based predictions of replicability},
 DOI={10.31222/osf.io/f675q},
 publisher={MetaArXiv},
 author={Gould, Elliot and Willcox, Aaron and Fraser, Hannah and Singleton Thorn, Felix and Wilkinson, David P},
 year={2021},
 month={Apr}
}


@book{Wickham2017R,
  abstract = {Learn how to use R to turn raw data into insight, knowledge, and understanding. This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience, R for Data Science is designed to get you doing data science as quickly as possible.

Authors Hadley Wickham and Garrett Grolemund guide you through the steps of importing, wrangling, exploring, and modeling your data and communicating the results. You'll get a complete, big-picture understanding of the data science cycle, along with basic tools you need to manage the details. Each section of the book is paired with exercises to help you practice what you've learned along the way.

You'll learn how to:

    Wrangle—transform your datasets into a form convenient for analysis
    Program—learn powerful R tools for solving data problems with greater clarity and ease
    Explore—examine your data, generate hypotheses, and quickly test them
    Model—provide a low-dimensional summary that captures true "signals" in your dataset
    Communicate—learn R Markdown for integrating prose, code, and results},
  added-at = {2018-06-18T21:23:34.000+0200},
  author = {Wickham, Hadley and Grolemund, Garrett},
  biburl = {https://www.bibsonomy.org/bibtex/22e6e5a8bda4b8020e3b4e90c5accf9a8/pbett},
  citeulike-article-id = {14263839},
  citeulike-linkout-0 = {http://r4ds.had.co.nz/},
  citeulike-linkout-1 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20&amp;path=ASIN/1491910399},
  citeulike-linkout-10 = {http://www.librarything.com/isbn/1491910399},
  citeulike-linkout-11 = {http://www.worldcat.org/oclc/979415716},
  citeulike-linkout-2 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21&amp;path=ASIN/1491910399},
  citeulike-linkout-3 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21&amp;path=ASIN/1491910399},
  citeulike-linkout-4 = {http://www.amazon.jp/exec/obidos/ASIN/1491910399},
  citeulike-linkout-5 = {http://www.amazon.co.uk/exec/obidos/ASIN/1491910399/citeulike00-21},
  citeulike-linkout-6 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20&path=ASIN/1491910399},
  citeulike-linkout-7 = {http://www.worldcat.org/isbn/1491910399},
  citeulike-linkout-8 = {http://books.google.com/books?vid=ISBN1491910399},
  citeulike-linkout-9 = {http://www.amazon.com/gp/search?keywords=1491910399&index=books&linkCode=qs},
  comment = {Available (section-by-section) online from http://r4ds.had.co.nz/},
  day = 05,
  edition = 1,
  howpublished = {Paperback},
  interhash = {660059a5b6b582a913488afb63e66907},
  intrahash = {2e6e5a8bda4b8020e3b4e90c5accf9a8},
  isbn = {1491910399},
  keywords = {visualisation textbook computing rpackage},
  month = jan,
  posted-at = {2017-04-03 14:44:51},
  priority = {2},
  publisher = {O'Reilly Media},
  timestamp = {2018-06-22T18:36:55.000+0200},
  title = {R for Data Science: Import, Tidy, Transform, Visualize, and Model Data},
  url = {http://r4ds.had.co.nz/},
  year = 2017
}


@Manual{R:2020,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2020},
    url = {https://www.R-project.org/}
  }

@misc{Fraser:2021,
 title={Predicting reliability through structured expert elicitation with repliCATS (Collaborative Assessments for Trustworthy Science)},
 DOI={10.31222/osf.io/2pczv},
 publisher={MetaArXiv},
 author={Fraser, Hannah and Bush, Martin and Wintle, Bonnie and Mody, Fallon and Smith, Eden T and Hanea, Anca and Gould, Elliot and Hemming, Victoria and Hamilton, Daniel G and Rumpff, Libby and Wilkinson, DP and Pearson, R and Singleton Thorn, F and Ashton, R and Willcox, A and Gray, CT and Head, A and Ross, M and Groenewegen, R and Marcoci, A and Vercammen, A and Parker, TH and Hoekstra, R and Nakagawa, S and Mandel, DR and {van Ravenzwaaij}, D and McBride, M and Sinnot, RO and Vesk, PA and Burgman, M and Fidler, Fiona},
 year={2021},
 month={Feb}
}


@Article{Goossens2008,
author = {Goossens, L.H.J. and Cooke, R.M. and Hale, A.R. and Rodic-Wiersma, L.J.},
title = {Fifteen years of expert judgement at TUDelft},
journal = {Safety Science},
volume = {46},
number = {2},
pages = {234–244},
year = {2008},
abstract = {},
location = {},
keywords = {}}

@Article{Isager2020,
author = {Isager, Peder Mortvedt and van Aert, R and Bahnik, S and Brandt, MJ and Desoto, KA and Ginner-Sorolla, R and Krueger, JI and Perugini, M and Ropovik, I and {van't Veer}, AE and Vranka, M and Lakens, D},
title = {Deciding what to replicate: A formal definition of "replication value" and a decision model for replication study selection.},
journal = {Journal of Informetrics},
volume = {13},
number = {2},
pages = {635–642},
year = {2020},
abstract = {Robust scientific knowledge is contingent upon replication of original findings. However, researchers who conduct replication studies face a difficult problem; there are many more studies in need of replication than there are funds available for replicating. To select studies for replication efficiently, we need to understand which studies are the most in need of replication. In other words, we need to understand which replication efforts have the highest expected utility. In this article we propose a general rule for study selection in replication research based on the replication value of the claims considered for replication. The replication value of a claim is defined as the maximum expected utility we could gain by replicating the claim, and is a function of (1) the value of being certain about the claim, and (2) uncertainty about the claim based on current evidence. We formalize this definition in terms of a causal decision model, utilizing concepts from decision theory and causal graph modeling. We discuss the validity of using replication value as a measure of expected utility gain, and we suggest approaches for deriving quantitative estimates of replication value.},
location = {},
keywords = {},
DOI = {https://doi.org/10.1037/met0000438}}


@article{Ebersole2016,
title = {Many Labs 3: Evaluating participant pool quality across the academic semester via replication},
journal = {Journal of Experimental Social Psychology},
volume = {67},
pages = {68-82},
year = {2016},
note = {Special Issue: Confirmatory},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2015.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0022103115300123},
author = {Charles R. Ebersole and Olivia E. Atherton and Aimee L. Belanger and Hayley M. Skulborstad and Jill M. Allen and Jonathan B. Banks and Erica Baranski and Michael J. Bernstein and Diane B.V. Bonfiglio and Leanne Boucher and Elizabeth R. Brown and Nancy I. Budiman and Athena H. Cairo and Colin A. Capaldi and Christopher R. Chartier and Joanne M. Chung and David C. Cicero and Jennifer A. Coleman and John G. Conway and William E. Davis and Thierry Devos and Melody M. Fletcher and Komi German and Jon E. Grahe and Anthony D. Hermann and Joshua A. Hicks and Nathan Honeycutt and Brandon Humphrey and Matthew Janus and David J. Johnson and Jennifer A. Joy-Gaba and Hannah Juzeler and Ashley Keres and Diana Kinney and Jacqeline Kirshenbaum and Richard A. Klein and Richard E. Lucas and Christopher J.N. Lustgraaf and Daniel Martin and Madhavi Menon and Mitchell Metzger and Jaclyn M. Moloney and Patrick J. Morse and Radmila Prislin and Timothy Razza and Daniel E. Re and Nicholas O. Rule and Donald F. Sacco and Kyle Sauerberger and Emily Shrider and Megan Shultz and Courtney Siemsen and Karin Sobocko and R. {Weylin Sternglanz} and Amy Summerville and Konstantin O. Tskhay and Zack {van Allen} and Leigh Ann Vaughn and Ryan J. Walker and Ashley Weinberg and John Paul Wilson and James H. Wirth and Jessica Wortman and Brian A. Nosek},
keywords = {Social psychology, Cognitive psychology, Replication, Participant pool, Individual differences, Sampling effects, Situational effects},
abstract = {The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N=2696) and with an online sample (N=737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences—conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.}
}

@Manual{dplyr2021,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller},
  year = {2021},
  note = {R package version 1.0.6},
  url = {https://CRAN.R-project.org/package=dplyr}
}

@Manual{magrittr2020,
  title = {magrittr: A Forward-Pipe Operator for R},
  author = {Stefan Milton Bache and Hadley Wickham},
  year = {2020},
  note = {R package version 2.0.1},
  url = {https://CRAN.R-project.org/package=magrittr}
}

@Article{tidyverse2019,
    title = {Welcome to the {tidyverse}},
    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
    year = {2019},
    journal = {Journal of Open Source Software},
    volume = {4},
    number = {43},
    pages = {1686},
    doi = {10.21105/joss.01686}
}

@Manual{purrr2020,
    title = {purrr: Functional Programming Tools},
    author = {Lionel Henry and Hadley Wickham},
    year = {2020},
    note = {R package version 0.3.4},
    url = {https://CRAN.R-project.org/package=purrr}
  }

@Book{ggplot2016,
  author = {Hadley Wickham},
  title = {ggplot2: Elegant Graphics for Data Analysis},
  publisher = {Springer-Verlag New York},
  year = {2016},
  isbn = {978-3-319-24277-4},
  url = {https://ggplot2.tidyverse.org}
}

@article{Yenni2019,
    doi = {10.1371/journal.pbio.3000125},
    author = {Yenni, Glenda M. AND Christensen, Erica M. AND Bledsoe, Ellen K. AND Supp, Sarah R. AND Diaz, Renata M. AND White, Ethan P. AND Ernest, S. K. Morgan},
    journal = {PLOS Biology},
    publisher = {Public Library of Science},
    title = {Developing a modern data workflow for regularly updated data},
    year = {2019},
    month = {01},
    volume = {17},
    url = {https://doi.org/10.1371/journal.pbio.3000125},
    pages = {1-12},
    abstract = {Over the past decade, biology has undergone a data revolution in how researchers collect data and the amount of data being collected. An emerging challenge that has received limited attention in biology is managing, working with, and providing access to data under continual active collection. Regularly updated data present unique challenges in quality assurance and control, data publication, archiving, and reproducibility. We developed a workflow for a long-term ecological study that addresses many of the challenges associated with managing this type of data. We do this by leveraging existing tools to 1) perform quality assurance and control; 2) import, restructure, version, and archive data; 3) rapidly publish new data in ways that ensure appropriate credit to all contributors; and 4) automate most steps in the data pipeline to reduce the time and effort required by researchers. The workflow leverages tools from software development, including version control and continuous integration, to create a modern data management system that automates the pipeline.},
    number = {1},

}


@Article{Arlidge2020,
author = {Arlidge, William N.S. and Alfaro-Shigueto, Joanna and Ibanez-Erquiaga, Bruno and Mangel, Jeffrey C. and Squires, Dale and Milner-Gulland, Eleanor J. },
title = {Evaluating elicited judgments of turtle captures for data‐limited fisheries management},
journal = {Conservation Science and Practice},
volume = {2},
number = {5},
pages = {},
year = {2020},
abstract = {},
location = {},
keywords = {}}

@Article{Camerer2018,
author = {Camerer, CF and Dreber, Anna and Holzmeister, Felix and Ho,{Teck-Hua} and Huber, Jurgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskill and Gampa, Anup, and Heikensten, Emma and Hummer, Lily and Taisuke, Imai and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, {Eric-Jan} and Wu, Hang},
title = {Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015},
journal = {naturecom},
volume = {},
number = {},
pages = {},
year = {2018},
abstract = {Being able to replicate scientific findings is crucial for scientific progress 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 2015 16, 17, 18, 19, 20...},
location = {},
keywords = {}}

@Article{aac4716,
author = {},
title = {Estimating the reproducibility of psychological science},
journal = {Science},
volume = {349},
number = {6251},
pages = {aac4716},
year = {2015},
doi = {10.1126/science.aac4716},
abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.}}

@Manual{veganpkg2020,
    title = {vegan: Community Ecology Package},
    author = {Jari Oksanen and F. Guillaume Blanchet and Michael Friendly and Roeland Kindt and Pierre Legendre and Dan McGlinn and Peter R. Minchin and R. B. O'Hara and Gavin L. Simpson and Peter Solymos and M. Henry H. Stevens and Eduard Szoecs and Helene Wagner},
    year = {2020},
    note = {R package version 2.5-7},
    url = {https://CRAN.R-project.org/package=vegan}
  }

@Manual{reprexpkg2020,
  title = {reprex: Prepare Reproducible Example Code via the Clipboard},
  author = {Jennifer Bryan and Jim Hester and David Robinson and Hadley Wickham},
  year = {2021},
  note = {R package version 2.0.0},
  url = {https://CRAN.R-project.org/package=reprex}
}

@misc{alipourfard2021,
 title={Systematizing Confidence in Open Research and Evidence (SCORE)},
 DOI={10.31235/osf.io/46mnb},
 publisher={SocArXiv},
 author={Alipourfard, Nazanin and Arendt, Beatrix and Benjamin, Daniel M and Benkler, Noam and Bishop, Michael M and Burstein, Mark and Bush, Martin and Caverlee, James and Chen, Yiling and Clark, Chae and Dreber, Anna and Errington, Timothy M. and Fidler, Fiona and Fox, Nicholas and Frank, Aaron and Fraser, Hannah and Friedman, Scott and Gelman, Ben and Gentile, James and Gordon, Michael and Griffin, Christopher and Gulden, Timothy and Hahn, Krystal and Hartman, Robert and Holzmeister, Felix and Hu, Xia and Johannesson, Magnus and Kezar, Lee and {Kline Struhl}, Melissa and Kuter, Ugur and Kwasnica, Anthony and Lee, {Dong-Ho} and Lerman, Kristina and Liu, Yang and Loomas, Zach and Luis, Bri and Magnusson, Ian and Bishop, Michael and Miske, Olivia and Mody, Fallon and Morstatter, Fred and Nosek, Brian A. and Parsons, Simon and Pennock, David and Pi, Haochen and Pujara, Jay and Rajtmajer, Sarah and Ren, Xiang and Salinas, Abel and Selvam, Ravi and Shipman, Frank and Silverstein, Priya and Sprenger, Amber and Squicciarini, Anna and Stratman, Stephen and Sun, Kexuan and Tikoo, Saatvik and Twardy, Charles R. and Tyner, Andrew and Viganola, Domenico and Wang, Juntao and Wilkinson, David and Wintle, Bonnie},
 year={2021},
 month={May}
}


@Incollection{wickham2017b,
  author = {Wickham, H and Grolemund, G},
  title = {Chapter 17: Iteration with Purr},
  booktitle = {R for Data Science},
  editor = {},
  publisher = {O'Reilly},
  address = {Sebastpool, Canada},
  pages = {313-344},
  year = {2017},
  abstract = {},
  keywords = {R}
}

@Manual{ggridges2021,
  title = {ggridges: Ridgeline Plots in "ggplot2"},
  author = {Claus O. Wilke},
  year = {2021},
  note = {R package version 0.5.3},
  url = {https://CRAN.R-project.org/package=ggridges},
}
