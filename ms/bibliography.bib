@book{CameronTrivedi2013,
  author    = {A. Colin Cameron and Pravin K. Trivedi},
  title     = {Regression Analysis of Count Data},
  year      = {2013},
  edition   = {2nd},
  publisher = {Cambridge University Press},
  address   = {Cambridge}
}

@book{ChambersHastie1992,
  editor    = {John M. Chambers and Trevor J. Hastie},
  title     = {Statistical Models in S},
  publisher = {Chapman \& Hall},
  year      = {1992},
  address   = {London}
}

@manual{Jackman2015,
  title  = {pscl: Classes and Methods for R Developed in the Political Science Computational Laboratory, Stanford University},
  author = {Simon Jackman},
  year   = {2015},
  note   = {R package version 1.4.9},
  url    = {https://CRAN.R-project.org/package=pscl}
}

@article{Mullahy1986,
  author  = {John Mullahy},
  title   = {Specification and Testing of Some Modified Count Data Models},
  year    = {1986},
  journal = {Journal of Econometrics},
  volume  = {33},
  number  = {3},
  pages   = {341--365},
  doi     = {10.1016/0304-4076(86)90002-3}
}

@book{McCullaghNelder1989,
  author    = {Peter McCullagh and John A. Nelder},
  title     = {Generalized Linear Models},
  edition   = {2nd},
  year      = {1989},
  publisher = {Chapman \& Hall},
  address   = {London},
  doi       = {10.1007/978-1-4899-3242-6}
}

@Article{Nakagawa2017,
author = {Nakagawa, S et al.},
title = {Meta-evaluation of meta-analysis: ten appraisal questions for biologists.},
journal = {BMC Biol},
volume = {15},
number = {1},
pages = {18},
year = {2017},
abstract = {Meta-analysis is a statistical procedure for analyzing the combined data from different studies, and can be a major source of concise up-to-date information. The overall conclusions of a meta-analysis, however, depend heavily on the quality of the meta-analytic process, and an appropriate evaluation of the quality of meta-analysis (meta-evaluation) can be challenging. We outline ten questions biologists can ask to critically appraise a meta-analysis. These questions could also act as simple and accessible guidelines for the authors of meta-analyses. We focus on meta-analyses using non-human species, which we term `biological' meta-analysis. Our ten questions are aimed at enabling a biologist to evaluate whether a biological meta-analysis embodies `mega-enlightenment', a `mega-mistake', or something in between.},
location = {Evolution \& Ecology Research Centre and School of Biological, Earth and Environmental Sciences, University of New South Wales, Sydney, NSW, 2052, Australia. s.nakagawa@unsw.edu.au. Diabetes and Metabolism Division, Garvan Institute of Medical Research, 384 Victoria Street, Darlinghurst, Sydney, NSW, 2010, Australia. s.nakagawa@unsw.edu.au. Evolution \& Ecology Research Centre and School of Biological, Earth and Environmental Sciences, University of New South Wales, Sydney, NSW, 2052, Australia. Charles Perkins Centre, University of Sydney, Sydney, NSW, 2006, Australia. School of Mathematics and Statistics, University of Sydney, Sydney, NSW, 2006, Australia. Evolution \& Ecology Research Centre and School of Biological, Earth and Environmental Sciences, University of New South Wales, Sydney, NSW, 2052, Australia.},
keywords = {}}



@manual{R,
  title        = {R: {A} Language and Environment for Statistical Computing},
  author       = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address      = {Vienna, Austria},
  year         = {2017},
  url          = {https://www.R-project.org/}
}

@article{StasinopoulosRigby2007,
  author  = {D. Mikis Stasinopoulos and Robert A. Rigby},
  title   = {Generalized Additive Models for Location Scale and Shape ({GAMLSS}) in R},
  journal = {Journal of Statistical Software},
  year    = {2007},
  volume  = {23},
  number  = {7},
  pages   = {1--46},
  doi     = {10.18637/jss.v023.i07}
}

@book{VenablesRipley2002,
  author    = {William N. Venables and Brian D. Ripley},
  title     = {Modern Applied Statistics with S},
  edition   = {4th},
  year      = {2002},
  pages     = {495},
  publisher = {Springer-Verlag},
  address   = {New York},
  doi       = {10.1007/978-0-387-21706-2}
}

@book{Wood2006,
  author    = {Simon N. Wood},
  title     = {Generalized Additive Models: An Introduction with R},
  year      = {2006},
  publisher = {Chapman \& Hall/CRC},
  address   = {Boca Raton}
}

@article{Yee2009,
  author  = {Thomas W. Yee},
  title   = {The VGAM Package for Categorical Data Analysis},
  journal = {Journal of Statistical Software},
  year    = {2010},
  volume  = {32},
  number  = {10},
  pages   = {1--34},
  doi     = {10.18637/jss.v032.i10}
}

@article{ZeileisKleiberJackman2008,
  author  = {Achim Zeileis and Christian Kleiber and Simon Jackman},
  title   = {Regression Models for Count Data in R},
  journal = {Journal of Statistical Software},
  year    = {2008},
  volume  = {27},
  number  = {8},
  pages   = {1--25},
  doi     = {10.18637/jss.v027.i08}
}


@article{hemming2017,
	title = {A practical guide to structured expert elicitation using the IDEA protocol},
	author = {{Hemming}, {Victoria} and {Burgman}, {Mark A.} and {Hanea}, {Anca M.} and {McBride}, {Marissa F.} and {Wintle}, {Bonnie C.}},
	editor = {{Anderson}, {Barbara}},
	year = {2017},
	month = {09},
	date = {2017-09-05},
	journal = {Methods in Ecology and Evolution},
	pages = {169--180},
	volume = {9},
	number = {1},
	doi = {10.1111/2041-210x.12857},
	url = {http://dx.doi.org/10.1111/2041-210x.12857},
	langid = {en}
}

@Article{Gordon2020,
author = {Gordon, Michael et al.},
title = {Are replication rates the same across academic fields? Community forecasts from the DARPA SCORE programme},
journal = {R Soc open sci},
volume = {7},
number = {7},
pages = {200566},
year = {2020},
abstract = {The Defense Advanced Research Projects Agency (DARPA) programme ‘Systematizing Confidence in Open Research and Evidence’ (SCORE) aims to generate confidence scores for a large number of research claims from empirical studies in the social and behavioural sciences. The confidence scores will provide a quantitative assessment of how likely a claim will hold up in an independent replication. To create the scores, we follow earlier approaches and use prediction markets and surveys to forecast replication outcomes. Based on an initial set of forecasts for the overall replication rate in SCORE and its dependence on the academic discipline and the time of publication, we show that participants expect replication rates to increase over time. Moreover, they expect replication rates to differ between fields, with the highest replication rate in economics (average survey response 58%), and the lowest in psychology and in education (average survey response of 42% for both fields). These results reveal insights into the academic community’s views of the replication crisis, including for research fields for which no large-scale replication studies have been undertaken yet.},
location = {},
keywords = {}}


@Article{Hanea2021,
 title={Mathematically aggregating experts' predictions of possible futures},
 url={osf.io/preprints/metaarxiv/rxmh7},
 DOI={10.31222/osf.io/rxmh7},
 publisher={MetaArXiv},
author = {Hanea, Anca and Wilkinson, David P and McBride, Marissa and Lyon, Aidan and van Ravenzwaaij, Don and Singleton Thorn, Felix and Gray, Charles T and Mandel, David R and Willcox, Aaron and Gould, Elliot and et al.},
 year={2021},
 month={Feb}
}

@misc{Gould2022,
 title={aggreCAT: An R Package for Mathematically Aggregating Expert Judgments},
 url={osf.io/preprints/metaarxiv/74tfv},
 DOI={10.31222/osf.io/74tfv},
 publisher={MetaArXiv},
 author={Gould, Elliot and Gray, Charles T and Willcox, Aaron and O'Dea, Rose E and Groenewegen, Rebecca and Wilkinson, David P},
 year={in prep.},
 month={Apr}
}

@Manual{R2JAGS,
title = {R2jags: Using R to Run 'JAGS'},
author = {Yu-Sung Su and Masanao Yajima},
year = {2020},
note = {R package version 0.6-1},
url = {https://CRAN.R-project.org/package=R2jags}
}

@Proceedings{Pearson2021,
author = {{Pearson}, {Ross} and {Fraser}, {Hannah} and {Bush}, {Martin} and {Mody}, {Fallon} and {Widjaja}, {Ivo} and {Head}, {Andy} and {Wilkinson}, {David Peter} and {Sinnott}, {Richard} and {Wintle}, {Bonnie} and {Burgman}, {Mark} and {Fidler}, {Fiona} and {Vesk}, {Peter}},
editor = {},
title = {Eliciting group judgements about replicability: a technical implementation of the IDEA Protocol},
booktitle = {Eliciting group judgements about replicability: a technical implementation of the IDEA Protocol},
volume = {},
publisher = {Hawaii International Conference on System Sciences},
address = {},
pages = {},
year = {2021},
abstract = {},
keywords = {},
url = {http://hdl.handle.net/10125/70666}}

@Article{Wickham:2014vp,
author = {Wickham, H},
title = {Tidy data},
journal = {Journal of Statistical Software},
volume = {59},
number = {10},
pages = {},
year = {2014},
abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
location = {},
keywords = {R; software}}

@Article{Wintle:2021,
author = {{Wintle}, B.C.},
title = {Predicting and reasoning about replicability using structured groups},
journal = {TBD},
volume = {},
number = {},
pages = {},
year = {2021},
abstract = {This paper explores judgements about the replicability of social and behavioural sciencesresearch, and what drives those judgements. Using a mixed methods approach, it draws on qualitative and quantitative data elicited using a structured iterative approach for elicitingjudgements from groups, called the IDEA protocol (‘Investigate’, ‘Discuss’, ‘Estimate’ and‘Aggregate’). Five groups of five people separately assessed the replicability of 25 ‘known-outcome’ claims. That is, social and behavioural science claims that have already been subject toat least one replication study. Specifically, participants assessed the probability that each of the25 research claims will replicate (i.e. a replication study would find a statistically significantresult in the same direction as the original study). In addition to their quantitative judgements,participants also outlined the reasoning behind their judgements. To start, we quantitativelyanalysed some possible correlates of predictive accuracy, such as self-rated understanding andexpertise in assessing each claim, and updating of judgements after feedback and discussion.Then we qualitatively analysed the reasoning data (i.e., the comments and justifications peopleprovided for their judgements) to explore the cues and heuristics used, and features of groupdiscussion that accompanied more and less accurate judgements},
location = {},
keywords = {}
}


@Article{Sutherland2018,
author = {Sutherland, William J. et al.},
title = {Qualitative methods for ecologists and conservation scientists},
journal = {Methods in Ecology and Evolution},
volume = {9},
number = {1},
pages = {7–9},
year = {2018},
abstract = {},
location = {},
keywords = {expert elicitation; focus groups; multi-criteria analysis; multi-criteria decision making; nominal group technique; policy making; qualitative interviews; qualitative research}}


@Article{Klein2014,
author = {Klein, Richard A. et al.},
title = {Investigating Variation in Replicability},
journal = {Social Psychology},
volume = {45},
number = {3},
pages = {142–152},
year = {2014},
abstract = {<jats:p> Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect – imagined contact reducing prejudice – showed weak support for replicability. And two effects – flag priming influencing conservatism and currency priming influencing system justification – did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect. </jats:p},
location = {},
keywords = {}}

@article{Klein2018ManyL2,
  title={Many Labs 2: Investigating Variation in Replicability Across Samples and Settings},
  author={R. A. Klein and M. Vianello and F. Hasselman and B. Adams and R. Adams and S. Alper and M. Aveyard and Jordan R Axt and Mayowa T. Babalola and {\v{S}}těp{\'a}n Bahn{\'i}k and R. Batra and M. Berkics and M. Bernstein and D. R. Berry and Olga Bialobrzeska and Evans Dami Binan and K. Bocian and M. Brandt and Robert Busching and Anna Redei and Huajian Cai and F. Cambier and K. Cantarero and Cheryl L Carmichael and F. C{\'e}ric and Jesse J Chandler and J. Chang and A. Chatard and E. Chen and Winnee Cheong and D. Cicero and S. Coen and Jennifer A Coleman and Brian Collisson and Morgan Conway and Katherine Corker and P. Curran and F. Cushman and Z. Dagona and Ilker Dalgar and A. Rosa and W. Davis and M. D. Bruijn and Leander De Schutter and T. Devos and M. D. Vries and Canay Doğulu and Nerisa Dozo and K. Dukes and Yarrow Dunham and K. Durrheim and C. Ebersole and J. Edlund and Anja Eller and Alexander S English and C. Finck and Natalia Frankowska and M. Freyre and Mike Friedman and E. Galliani and Joshua Chiroma Gandi and Tanuka Ghoshal and S. Giessner and Tripat Gill and Timo Gnambs and {\'A}ngel G{\'o}mez and R. Gonzalez and J. Graham and Jon E. Grahe and Ivan Grahek and Eva Green and Kakul Hai and M. Haigh and Elizabeth L. Haines and Michael P. Hall and Marie E. Heffernan and J. Hicks and P. Houdek and Jeffrey R. Huntsinger and H. Huynh and H. Ijzerman and Yoel Inbar and {\AA}. Innes-Ker and William Jimenez-Leal and Melissa-Sue John and Jennifer A. Joy-Gaba and R. Kamiloğlu and H. Kappes and Serdar Karabati and H. Karick and Victor N. Keller and Anna Kende and Nicolas Kervyn and G. Kne{\v{z}}evi{\'c} and C. Kovacs and Lacy E Krueger and G. Kurapov and J. Kurtz and D. Lakens and Ljiljana B. Lazarevi{\'c} and C. Levitan and Neil Lewis and Samuel Lins and Nikolette P. Lipsey and Joy E Losee and E. Maassen and Angela T Maitner and Winfrida Malingumu and Robyn K Mallett and Satia A. Marotta and Janko Međedovi{\'c} and Fernando Mena-Pacheco and T. Milfont and Wendy L. Morris and S. Murphy and A. Myachykov and N. Neave and K. Neijenhuijs and A. J. Nelson and F{\'e}lix Neto and Austin Lee Nichols and A. Ocampo and S. O'Donnell and Haruka Oikawa and M. Oikawa and E. Ong and G{\'a}bor Orosz and Malgorzata Osowiecka and Grant Packard and Rolando P{\'e}rez-S{\'a}nchez and B. Petrovi{\'c} and Ronaldo Pilati and B. Pinter and L. Podesta and Gabrielle Pogge and M. Pollmann and Abraham M. Rutchick and Patricio Saavedra and Alexander K Saeri and E. Salomon and Kathleen Schmidt and Felix D. Sch{\"o}nbrodt and M. Sekerdej and David Sirlop{\'u} and Jeanine L. M. Skorinko and M. A. Smith and Vanessa Smith-Castro and K. Smolders and Agata Sobkow and W. Sowden and Philipp Spachtholz and M. Srivastava and Troy G Steiner and J. Stouten and Chris N. H. Street and Oskar K. Sundfelt and S. Szeto and Ewa Szumowska and Andrew C. W. Tang and Norbert K Tanzer and Morgan J. Tear and Jordan E Theriault and M. Thomae and David Torres and J. Traczyk and Joshua M. Tybur and A. Ujhelyi and R. C. M. Aert and M. V. Assen and Marije van der Hulst and P. V. Lange and A. E. V. Veer and Alejandro Echeverr{\'i}a and L. Vaughn and A. V{\'a}zquez and L. D. Vega and Catherine Verniers and M. Verschoor and Ingrid Voermans and M. Vranka and C. A. Welch and A. Wichman and L. Williams and M. Wood and Julie A. Woodzicka and M. K. Wronska and L. Young and J. Zelenski and Zeng Zhi-jia and Brian A. Nosek},
  journal={Advances in Methods and Practices in Psychological Science},
  year={2018},
  volume={1},
  pages={443 - 490}
}


@Article{Gould2021a,
 title={Using model-based predictions to inform the mathematical aggregation of human-based predictions of replicability},
 url={osf.io/preprints/metaarxiv/f675q},
 DOI={10.31222/osf.io/f675q},
 publisher={MetaArXiv},
 author={Gould, Elliot and Willcox, Aaron and Fraser, Hannah and Singleton Thorn, Felix and Wilkinson, David P},
 year={2021},
 month={Apr}
}


@book{Wickham2017R,
  abstract = {Learn how to use R to turn raw data into insight, knowledge, and understanding. This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience, R for Data Science is designed to get you doing data science as quickly as possible.

Authors Hadley Wickham and Garrett Grolemund guide you through the steps of importing, wrangling, exploring, and modeling your data and communicating the results. You'll get a complete, big-picture understanding of the data science cycle, along with basic tools you need to manage the details. Each section of the book is paired with exercises to help you practice what you've learned along the way.

You'll learn how to:

    Wrangle—transform your datasets into a form convenient for analysis
    Program—learn powerful R tools for solving data problems with greater clarity and ease
    Explore—examine your data, generate hypotheses, and quickly test them
    Model—provide a low-dimensional summary that captures true "signals" in your dataset
    Communicate—learn R Markdown for integrating prose, code, and results},
  added-at = {2018-06-18T21:23:34.000+0200},
  author = {Wickham, Hadley and Grolemund, Garrett},
  biburl = {https://www.bibsonomy.org/bibtex/22e6e5a8bda4b8020e3b4e90c5accf9a8/pbett},
  citeulike-article-id = {14263839},
  citeulike-linkout-0 = {http://r4ds.had.co.nz/},
  citeulike-linkout-1 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20&amp;path=ASIN/1491910399},
  citeulike-linkout-10 = {http://www.librarything.com/isbn/1491910399},
  citeulike-linkout-11 = {http://www.worldcat.org/oclc/979415716},
  citeulike-linkout-2 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21&amp;path=ASIN/1491910399},
  citeulike-linkout-3 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21&amp;path=ASIN/1491910399},
  citeulike-linkout-4 = {http://www.amazon.jp/exec/obidos/ASIN/1491910399},
  citeulike-linkout-5 = {http://www.amazon.co.uk/exec/obidos/ASIN/1491910399/citeulike00-21},
  citeulike-linkout-6 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20&path=ASIN/1491910399},
  citeulike-linkout-7 = {http://www.worldcat.org/isbn/1491910399},
  citeulike-linkout-8 = {http://books.google.com/books?vid=ISBN1491910399},
  citeulike-linkout-9 = {http://www.amazon.com/gp/search?keywords=1491910399&index=books&linkCode=qs},
  comment = {Available (section-by-section) online from http://r4ds.had.co.nz/},
  day = 05,
  edition = 1,
  howpublished = {Paperback},
  interhash = {660059a5b6b582a913488afb63e66907},
  intrahash = {2e6e5a8bda4b8020e3b4e90c5accf9a8},
  isbn = {1491910399},
  keywords = {visualisation textbook computing rpackage},
  month = jan,
  posted-at = {2017-04-03 14:44:51},
  priority = {2},
  publisher = {O'Reilly Media},
  timestamp = {2018-06-22T18:36:55.000+0200},
  title = {R for Data Science: Import, Tidy, Transform, Visualize, and Model Data},
  url = {http://r4ds.had.co.nz/},
  year = 2017
}


@Manual{R:2020,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2020},
    url = {https://www.R-project.org/}
  }

@misc{Fraser:2021,
 title={Predicting reliability through structured expert elicitation with repliCATS (Collaborative Assessments for Trustworthy Science)},
 url={osf.io/preprints/metaarxiv/2pczv},
 DOI={10.31222/osf.io/2pczv},
 publisher={MetaArXiv},
 author={Fraser, Hannah and Bush, Martin and Wintle, Bonnie and Mody, Fallon and Smith, Eden T and Hanea, Anca and Gould, Elliot and Hemming, Victoria and Hamilton, Daniel G and Rumpff, Libby and et al.},
 year={2021},
 month={Feb}
}


@Article{Goossens2008,
author = {Goossens, L.H.J. et al.},
title = {Fifteen years of expert judgement at TUDelft},
journal = {Safety Science},
volume = {46},
number = {2},
pages = {234–244},
year = {2008},
abstract = {},
location = {},
keywords = {}}

@Article{Isager2020,
author = {Isager, Peder Mortvedt et al.},
title = {Deciding what to replicate: A formal definition of “replication value” and a decision model for replication study selection.},
journal = {Journal of Informetrics},
volume = {13},
number = {2},
pages = {635–642},
year = {2020},
abstract = {Robust scientific knowledge is contingent upon replication of original findings. However, researchers who conduct replication studies face a difficult problem; there are many more studies in need of replication than there are funds available for replicating. To select studies for replication efficiently, we need to understand which studies are the most in need of replication. In other words, we need to understand which replication efforts have the highest expected utility. In this article we propose a general rule for study selection in replication research based on the replication value of the claims considered for replication. The replication value of a claim is defined as the maximum expected utility we could gain by replicating the claim, and is a function of (1) the value of being certain about the claim, and (2) uncertainty about the claim based on current evidence. We formalize this definition in terms of a causal decision model, utilizing concepts from decision theory and causal graph modeling. We discuss the validity of using replication value as a measure of expected utility gain, and we suggest approaches for deriving quantitative estimates of replication value.},
location = {},
keywords = {}}

@Article{Ebersole2016,
author = {Ebersole, Charles R. et al.},
title = {Many Labs 3: Evaluating participant pool quality across the academic semester via replication},
journal = {Journal of Experimental Social Psychology},
volume = {67},
number = {},
pages = {68–82},
year = {2016}
}

@Manual{dplyr2021,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller},
  year = {2021},
  note = {R package version 1.0.6},
  url = {https://CRAN.R-project.org/package=dplyr}
}

@Manual{magrittr2020,
  title = {magrittr: A Forward-Pipe Operator for R},
  author = {Stefan Milton Bache and Hadley Wickham},
  year = {2020},
  note = {R package version 2.0.1},
  url = {https://CRAN.R-project.org/package=magrittr}
}

@Article{tidyverse2019,
    title = {Welcome to the {tidyverse}},
    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain François and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill Müller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
    year = {2019},
    journal = {Journal of Open Source Software},
    volume = {4},
    number = {43},
    pages = {1686},
    doi = {10.21105/joss.01686}
}

@Manual{purrr2020,
    title = {purrr: Functional Programming Tools},
    author = {Lionel Henry and Hadley Wickham},
    year = {2020},
    note = {R package version 0.3.4},
    url = {https://CRAN.R-project.org/package=purrr}
  }

@Book{ggplot2016,
  author = {Hadley Wickham},
  title = {ggplot2: Elegant Graphics for Data Analysis},
  publisher = {Springer-Verlag New York},
  year = {2016},
  isbn = {978-3-319-24277-4},
  url = {https://ggplot2.tidyverse.org}
}

@Article{Yenni2019,
author = {Yenni, GM et al.},
title = {Developing a modern data workflow for regularly updated data.},
journal = {PLoS Biol},
volume = {17},
number = {1},
pages = {e3000125},
year = {2019},
abstract = {Over the past decade, biology has undergone a data revolution in how researchers collect data and the amount of data being collected. An emerging challenge that has received limited attention in biology is managing, working with, and providing access to data under continual active collection. Regularly updated data present unique challenges in quality assurance and control, data publication, archiving, and reproducibility. We developed a workflow for a long-term ecological study that addresses many of the challenges associated with managing this type of data. We do this by leveraging existing tools to 1) perform quality assurance and control; 2) import, restructure, version, and archive data; 3) rapidly publish new data in ways that ensure appropriate credit to all contributors; and 4) automate most steps in the data pipeline to reduce the time and effort required by researchers. The workflow leverages tools from software development, including version control and continuous integration, to create a modern data management system that automates the pipeline.},
location = {Department of Wildlife Ecology and Conservation, University of Florida, Gainesville, Florida, United States of America. School of Natural Resources and the Environment, University of Florida, Gainesville, Florida, United States of America. Data Analytics Program, Denison University, Granville, Ohio, United States of America. School of Natural Resources and the Environment, University of Florida, Gainesville, Florida, United States of America. Department of Wildlife Ecology and Conservation, University of Florida, Gainesville, Florida, United States of America. Informatics Institute, University of Florida, Gainesville, Florida, United States of America. Department of Wildlife Ecology and Conservation, University of Florida, Gainesville, Florida, United States of America. Biodiversity Institute, University of Florida, Gainesville, Florida, United States of America.},
keywords = {}}

@Article{Arlidge2020,
author = {Arlidge, William N. S. et al.},
title = {Evaluating elicited judgments of turtle captures for data‐limited fisheries management},
journal = {Conservation Science and Practice},
volume = {2},
number = {5},
pages = {},
year = {2020},
abstract = {},
location = {},
keywords = {}}


@Article{Camerer2018,
author = {Camerer, CF et al.},
title = {Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015},
journal = {naturecom},
volume = {},
number = {},
pages = {},
year = {2018},
abstract = {Being able to replicate scientific findings is crucial for scientific progress 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 2015 16, 17, 18, 19, 20...},
location = {},
keywords = {}}

@Book{aac4716,
author = {},
title = {Estimating the reproducibility of psychological science},
volume = {349(6251)},
pages = {},
editor = {},
publisher = {American Association for the Advancement of Science},
address = {},
year = {2015},
abstract = {One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.Science, this issue 10.1126/science.aac4716INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P \&lt;05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that \textquotedblleftwe already know this\textquotedblright belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
keywords = {}}

@Manual{veganpkg2020,
    title = {vegan: Community Ecology Package},
    author = {Jari Oksanen and F. Guillaume Blanchet and Michael Friendly and Roeland Kindt and Pierre Legendre and Dan McGlinn and Peter R. Minchin and R. B. O'Hara and Gavin L. Simpson and Peter Solymos and M. Henry H. Stevens and Eduard Szoecs and Helene Wagner},
    year = {2020},
    note = {R package version 2.5-7},
    url = {https://CRAN.R-project.org/package=vegan}
  }

@Manual{reprexpkg2020,
  title = {reprex: Prepare Reproducible Example Code via the Clipboard},
  author = {Jennifer Bryan and Jim Hester and David Robinson and Hadley Wickham},
  year = {2021},
  note = {R package version 2.0.0},
  url = {https://CRAN.R-project.org/package=reprex}
}

@misc{alipourfard2021,
 title={Systematizing Confidence in Open Research and Evidence (SCORE)},
 url={osf.io/preprints/socarxiv/46mnb},
 DOI={10.31235/osf.io/46mnb},
 publisher={SocArXiv},
 author={Alipourfard, Nazanin and Arendt, Beatrix and Benjamin, Daniel M and Benkler, Noam and Bishop, Michael M and Burstein, Mark and Bush, Martin and Caverlee, James and Chen, Yiling and Clark, Chae and et al.},
 year={2021},
 month={May}
}

@Incollection{wickham2017b,
  author = {Wickham, H and Grolemund, G},
  title = {Chapter 17: Iteration with Purr},
  booktitle = {R for Data Science},
  editor = {},
  publisher = {O'Reilly},
  address = {Sebastpool, Canada},
  pages = {313-344},
  year = {2017},
  abstract = {},
  keywords = {R}
}

@Manual{ggridges2021,
  title = {ggridges: Ridgeline Plots in 'ggplot2'},
  author = {Claus O. Wilke},
  year = {2021},
  note = {R package version 0.5.3},
  url = {https://CRAN.R-project.org/package=ggridges},
}
